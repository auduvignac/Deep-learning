{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage par renforcement\n",
    "\n",
    "« L’apprentissage par renforcement (en anglais, reinforcement learning, ou RL) est la branche de l’apprentissage automatique qui consiste à apprendre comment un agent doit se comporter dans un environnement de manière à maximiser une récompense. Naturellement, l’apprentissage par renforcement profond restreint la méthode d’apprentissage à l’apprentissage profond. »\n",
    "\n",
    "Eugene Charniak, *Introduction au Deep Learning*, p. 105 ([« Introduction au Deep Learning, Eugene Charniak, Editions Dunod »](https://www.dunod.com/sciences-techniques/introduction-au-deep-learning))\n",
    "\n",
    "Dans le cadre de ce *notebook*, nous allons nous intéresser à un problème classique de l'apprentissage par renforcement : le *FrozenLake*.\n",
    "\n",
    "## Le problème du *FrozenLake*\n",
    "\n",
    "### Présetation du contexte\n",
    "\n",
    "Le *FrozenLake* est un environnement de type *gridworld* où l'objectif est de trouver un chemin sûr à travers une surface gelée, en évitant les cases *Hole* (H) et en atteignant la case *Goal* (G).\n",
    "\n",
    "L'environnement est représenté par une grille de 4x4 cases, où chaque case peut être soit :\n",
    "- S (Start) : le point de départ ;\n",
    "- F (Frozen) : une case sûre ;\n",
    "- H (Hole) : une case dangereuse ;\n",
    "- G (Goal) : la case d'arrivée (*i.e* la case à atteindre).\n",
    "\n",
    "L'agent peut se déplacer dans les quatre directions (haut, bas, gauche, droite) et l'objectif est d'atteindre la case *Goal* en évitant les cases *Hole*. L'agent reçoit une récompense de 1 lorsqu'il atteint la case *Goal* et une récompense de 0 dans tous les autres cas.\n",
    "\n",
    "La figure ci-dessous illustre un exemple de *FrozenLake* :\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/auduvignac/Deep-learning/refs/heads/main/Projet/figures/Reinforcement_learning/frozen_lake.png\" alt=\"Frozen Lake\"\n",
    "  width=\"40%\" height=\"40%\"/>\n",
    "</p>\n",
    "\n",
    "### Idées intuitives\n",
    "\n",
    "#### Valeur d'un état (*State Value* : $V(s)$)\n",
    "\n",
    "Pour résoudre ce problème, l'agent doit apprendre à naviguer dans l'environnement en prenant des décisions qui maximisent la récompense totale. Pour ce faire, il doit apprendre une politique qui lui permet de choisir la meilleure action à prendre dans chaque état.\n",
    "\n",
    "L'apprentissage par renforcement est basé sur le concept de *reward* (récompense) qui est une mesure de la qualité de l'action prise par l'agent. L'objectif de l'agent est de maximiser la récompense totale qu'il reçoit.\n",
    "\n",
    "En vue de décrire le problème, posons les éléments suivants :\n",
    "- s (*state*) : l'état actuel de l'agent ;\n",
    "- a (*action*) : l'action prise par l'agent ;\n",
    "- r (*reward*) la récompense reçue par l'agent ;\n",
    "- s' (*next state*) l'état suivant de l'agent.\n",
    "\n",
    "**L'objectif de l'agent est de maximiser la récompense totale qu'il reçoit en prenant des actions qui maximisent la récompense à chaque étape.**\n",
    "\n",
    "Pour quantitifier la valeur d'un état à un instant donné, définissons la fonction de valeur d'état $V(s)$ **qui représente la récompense totale que l'agent peut s'attendre à recevoir à partir de l'état $s$**.\n",
    "\n",
    "**Exemple concret :**\n",
    "\n",
    "- L’état G (*Goal*) a $V(G) = 1$ car c’est l’objectif ;\n",
    "- Un état très proche du but aura une valeur $V(s)$ élevée ;\n",
    "- Un état près d’un trou (*Hole*) aura une $V(s)$ faible car le risque est élevé ;\n",
    "- Un état éloigné du but aura une $V(s)$ proche de 0 car il est incertain d’atteindre l’objectif.\n",
    "\n",
    "#### Fonction de valeur d'action (*Action Value* : $Q(s, a)$)\n",
    "\n",
    "La fonction $Q(s, a)$ permet de quantitifier la valeur d'aller vers le haut, le bas, à droite ou à gauche depuis un état donné. En d'autres termes, **il s'agit de la récompense attendue à une état $s$ donné et que l’action a est choisie.**\n",
    "\n",
    "#### Exemple concret\n",
    "\n",
    "La calcul de $V$ et $Q$ nécessite de parcourir l'ensemble des états $s$ et de recalculer $V(s)$.\n",
    "\n",
    "Le mode de programmation de ce jeu induit qu’une action, par exemple aller à gauche nous conduit avec une égale probabilité dans chacun des états adjacents, à l’exception de l’exact opposé (ici, aller à droite) : le terrain est donc très glissant. Si une action aboutissait à nous faire sortir du lac, elle nous laisserait au lieu de cela à l’emplacement de départ.\n",
    "\n",
    "L'environnement du *Frozen Lake* est stochastique, c'est-à-dire que l'agent ne peut pas prédire exactement le prochain état. Par conséquent, il doit apprendre à partir de l'expérience en explorant l'environnement. En effet, **si l'agent décide d'aller à gauche, il est possible qu'il dévie avec une probabilité définie**, en l'occurrence $\\frac{1}{3}$, puisqu'il y a trois directions possibles :\n",
    "\n",
    "- Action réussie (vers la gauche) : état 0 avec une probabilité de $\\frac{1}{3}$ ;\n",
    "- Glissade vers une autre direction (vers le bas) : état 5 avec une probabilité de $\\frac{1}{3}$ ;\n",
    "- Bloqué par le bord du lac (rester sur place) : état 1 avec une probabilité de $\\frac{1}{3}$.\n",
    "\n",
    "La figure ci-dessous illustre les états possibles pour l'état 1 :\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/auduvignac/Deep-learning/refs/heads/main/Projet/figures/Reinforcement_learning/frozen_lake_exemple_state_1.png\" alt=\"Frozen Lake\"\n",
    "  width=\"40%\" height=\"40%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "il est nécessaire de calculer $Q(1, a)$ pour chacune des quatre actions puis d'affecter à $V(1)$ le maximum des quatre valeurs $Q$.\n",
    "\n",
    "\n",
    "\n",
    "Nous allons donc calculer $Q(1, a)$ pour chacune des actions possibles :\n",
    "\n",
    "1. Déplacement vers la gauche.\n",
    "$$\n",
    "\\begin{align*}\n",
    "Q(1, g) &= \\frac{1}{3} \\times (0 + 0.9 ∗ 0) + \\frac{1}{3} ∗ (0 + 0.9 ∗ 0) + \\frac{1}{3} ∗ (0 + 0.9 ∗ 0)\n",
    "        &= \n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annexe 1 : Génération des images d'illustration du *FrozenLake*\n",
    "\n",
    "Le code ci-dessous permet de générer l'image qui met en évidence l'état 1 et ses transitions possibles lorsqu'on tente de se déplacer vers la gauche.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# URL de l'image sur GitHub\n",
    "image_url = \"https://raw.githubusercontent.com/auduvignac/Deep-learning/refs/heads/main/Projet/figures/Reinforcement_learning/frozen_lake.png\"\n",
    "\n",
    "# Charger l'image depuis l'URL\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Définir la taille de la figure\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "# Afficher l'image en arrière-plan\n",
    "ax.imshow(img)\n",
    "\n",
    "# Définir la grille (4x4) et les coordonnées des cases\n",
    "grid_size = 4\n",
    "cell_size = img.size[0] / grid_size  # Assumer une image carrée\n",
    "\n",
    "# États concernés\n",
    "state_0 = (0, 0)  # Ligne 0, colonne 0 (indexation à partir de 0)\n",
    "state_1 = (0, 1)  # Ligne 0, colonne 1\n",
    "state_2 = (0, 2)  # Ligne 0, colonne 0\n",
    "state_5 = (1, 1)  # Ligne 1, colonne 1\n",
    "\n",
    "# Ajouter des annotations pour les états accessibles\n",
    "state_positions = {state_1: \"État 1\", state_0: \"État 0\", state_5: \"État 5\"}\n",
    "for (row, col), label in state_positions.items():\n",
    "    x, y = col * cell_size, row * cell_size\n",
    "    ax.add_patch(patches.Rectangle((x, y), cell_size, cell_size, fill=False, edgecolor=\"yellow\", linewidth=3))\n",
    "    ax.text(x + cell_size / 2, y + cell_size / 2, label, fontsize=12, color=\"black\", ha=\"center\", va=\"center\", bbox=dict(facecolor=\"white\", alpha=0.7))\n",
    "\n",
    "# Ajouter les probabilités de transition\n",
    "arrow_params = dict(facecolor=\"blue\", edgecolor=\"blue\", arrowstyle=\"->\", linewidth=2, alpha=0.8)\n",
    "\n",
    "# De État 1 vers État 0 (probabilité 0.33)\n",
    "ax.annotate(\"\", xy=(state_0[1] * cell_size + cell_size / 2, state_0[0] * cell_size + cell_size / 2),\n",
    "             xytext=(state_1[1] * cell_size + cell_size / 2, state_1[0] * cell_size + cell_size / 2),\n",
    "             arrowprops=arrow_params)\n",
    "ax.text((state_0[1] + state_1[1]) / 2 * cell_size, (state_0[0] + state_1[0]) / 2 * cell_size,\n",
    "        \"0.33\", fontsize=12, color=\"blue\", ha=\"center\", bbox=dict(facecolor=\"white\", alpha=0.7))\n",
    "\n",
    "# De État 1 vers État 5 (probabilité 0.33)\n",
    "ax.annotate(\"\", xy=(state_5[1] * cell_size + cell_size / 2, state_5[0] * cell_size + cell_size / 2),\n",
    "             xytext=(state_1[1] * cell_size + cell_size / 2, state_1[0] * cell_size + cell_size / 2),\n",
    "             arrowprops=arrow_params)\n",
    "ax.text((state_5[1] + state_1[1]) / 2 * cell_size, (state_5[0] + state_1[0]) / 2 * cell_size,\n",
    "        \"0.33\", fontsize=12, color=\"blue\", ha=\"center\", bbox=dict(facecolor=\"white\", alpha=0.7))\n",
    "\n",
    "# De État 1 vers lui-même (bord bloqué) (probabilité 0.33)\n",
    "ax.text(state_1[1] * cell_size + cell_size / 2, state_1[0] * cell_size - 10,\n",
    "        \"0.33 (reste sur place)\", fontsize=12, color=\"blue\", ha=\"center\", bbox=dict(facecolor=\"white\", alpha=0.7))\n",
    "\n",
    "# De État 1 vers État 2 (probabilité 0.33)\n",
    "ax.annotate(\"\", xy=(state_2[1] * cell_size + cell_size / 2, state_2[0] * cell_size + cell_size / 2),\n",
    "             xytext=(state_1[1] * cell_size + cell_size / 2, state_1[0] * cell_size + cell_size / 2),\n",
    "             arrowprops=arrow_params)\n",
    "ax.text((state_2[1] + state_1[1]) / 2 * cell_size, (state_5[0] + state_1[0]) / 2 * cell_size,\n",
    "        \"0.33\", fontsize=12, color=\"blue\", ha=\"center\", bbox=dict(facecolor=\"white\", alpha=0.7))\n",
    "\n",
    "# Masquer les axes\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Sauvegarder et afficher l'image annotée\n",
    "plt.savefig(\"frozen_lake_annotated.png\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
